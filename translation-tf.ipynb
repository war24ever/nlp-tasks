{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"aRHkFNbcu0qN","outputId":"9d6b3065-3bf2-4264-c932-5da3e5b16c7c"},"outputs":[{"name":"stdout","output_type":"stream","text":["4.21.0.dev0\n"]}],"source":["import transformers\n","\n","print(transformers.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OH4naqt-u0qP"},"outputs":[],"source":["model_checkpoint = \"Helsinki-NLP/opus-mt-en-ROMANCE\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IreSlFmlIrIm","colab":{"referenced_widgets":["3cead726a3164d67847ade363d465233"]},"outputId":"71b8c468-6931-4950-ff8c-8c186e4672a1"},"outputs":[{"name":"stderr","output_type":"stream","text":["Reusing dataset wmt16 (/home/matt/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/28ebdf8cf22106c2f1e58b2083d4b103608acd7bfdb6b14313ccd9e5bc8c313a)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3cead726a3164d67847ade363d465233","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","from evaluate import load\n","\n","raw_datasets = load_dataset(\"wmt16\", \"ro-en\")\n","metric = load(\"sacrebleu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWiVUF0jIrIv","outputId":"35e3ea43-f397-4a54-c90c-f2cf8d36873e"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['translation'],\n","        num_rows: 610320\n","    })\n","    validation: Dataset({\n","        features: ['translation'],\n","        num_rows: 1999\n","    })\n","    test: Dataset({\n","        features: ['translation'],\n","        num_rows: 1999\n","    })\n","})"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["raw_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6HrpprwIrIz","outputId":"d7670bc0-42e4-4c09-8a6a-5c018ded7d95"},"outputs":[{"data":{"text/plain":["{'translation': {'en': 'Membership of Parliament: see Minutes',\n","  'ro': 'Componenţa Parlamentului: a se vedea procesul-verbal'}}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["raw_datasets[\"train\"][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3j8APAoIrI3"},"outputs":[],"source":["import datasets\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","\n","def show_random_elements(dataset, num_examples=5):\n","    assert num_examples <= len(\n","        dataset\n","    ), \"Can't pick more elements than there are in the dataset.\"\n","    picks = []\n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(dataset) - 1)\n","        while pick in picks:\n","            pick = random.randint(0, len(dataset) - 1)\n","        picks.append(pick)\n","\n","    df = pd.DataFrame(dataset[picks])\n","    for column, typ in dataset.features.items():\n","        if isinstance(typ, datasets.ClassLabel):\n","            df[column] = df[column].transform(lambda i: typ.names[i])\n","    display(HTML(df.to_html()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SZy5tRB_IrI7","outputId":"ba8f2124-e485-488f-8c0c-254f34f24f13"},"outputs":[{"data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>translation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>{'en': '\"Kosovo does not have not a positive image mainly because (the media portrays) the Serbs living in ghettoes ... and NATO helping Albanians displace the Serbs from Kosovo,\" Chukov said.', 'ro': '\"Kosovo nu are o imagine pozitivă mai ales din cauza faptului că (presa arată) că sârbii trăiesc în ghetto-uri ... şi NATO îi ajută pe albanezi să strămute sârbii din Kosovo\", a spus Chukov.'}</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>{'en': 'They also signed a memorandum of understanding on diplomatic consultations.', 'ro': 'Aceştia au semnat de asemenea un protocol de acord cu privire la consultaţiile diplomatice.'}</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>{'en': 'EU Commissioner for Home Affairs Cecilia Malmstrom said on Monday (September 20th) that Albania has made significant progress in meeting requirements for visa-free travel.', 'ro': 'Comisarul UE pentru afaceri interne, Cecilia Malmstrom, a declarat luni (20 septembrie) că Albania a făcut progrese semnificative în întrunirea condiţiilor pentru liberalizarea vizelor.'}</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>{'en': '13.', 'ro': '13.'}</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>{'en': 'But, in principle, thank you very much for what was, for me, too, a very interesting debate, and all the best.', 'ro': 'Dar, în principiu, vă mulţumesc foarte mult pentru această dezbatere care a fost foarte interesantă pentru mine şi vă urez toate cele bune.'}</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["show_random_elements(raw_datasets[\"train\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5o4rUteaIrI_","outputId":"18038ef5-554c-45c5-e00a-133b02ec10f1"},"outputs":[{"data":{"text/plain":["EvaluationModule(name: \"sacrebleu\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\n","Produces BLEU scores along with its sufficient statistics\n","from a source against one or more references.\n","\n","Args:\n","    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n","    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n","    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n","        - `'none'`: no smoothing\n","        - `'floor'`: increment zero counts\n","        - `'add-k'`: increment num/denom by k for n>1\n","        - `'exp'`: exponential decay\n","    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n","    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n","        - `'none'`: No tokenization.\n","        - `'zh'`: Chinese tokenization.\n","        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n","        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n","        - `'char'`: Language-agnostic character-level tokenization.\n","        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n","    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n","    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n","    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n","\n","Returns:\n","    'score': BLEU score,\n","    'counts': Counts,\n","    'totals': Totals,\n","    'precisions': Precisions,\n","    'bp': Brevity penalty,\n","    'sys_len': predictions length,\n","    'ref_len': reference length,\n","\n","Examples:\n","\n","    Example 1:\n","        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n","        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n","        >>> sacrebleu = evaluate.load(\"sacrebleu\")\n","        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n","        >>> print(list(results.keys()))\n","        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n","        >>> print(round(results[\"score\"], 1))\n","        100.0\n","\n","    Example 2:\n","        >>> predictions = [\"hello there general kenobi\",\n","        ...                 \"on our way to ankh morpork\"]\n","        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n","        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n","        >>> sacrebleu = evaluate.load(\"sacrebleu\")\n","        >>> results = sacrebleu.compute(predictions=predictions,\n","        ...                             references=references)\n","        >>> print(list(results.keys()))\n","        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n","        >>> print(round(results[\"score\"], 1))\n","        39.8\n","\"\"\", stored examples: 0)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["metric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XN1Rq0aIrJC","outputId":"a4405435-a8a9-41ff-9f79-a13077b587c7"},"outputs":[{"data":{"text/plain":["{'score': 0.0,\n"," 'counts': [4, 2, 0, 0],\n"," 'totals': [4, 2, 0, 0],\n"," 'precisions': [100.0, 100.0, 0.0, 0.0],\n"," 'bp': 1.0,\n"," 'sys_len': 4,\n"," 'ref_len': 4}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["fake_preds = [\"hello there\", \"general kenobi\"]\n","fake_labels = [[\"hello there\"], [\"general kenobi\"]]\n","metric.compute(predictions=fake_preds, references=fake_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eXNLu_-nIrJI"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dz6uUOfUu0qS"},"outputs":[],"source":["if \"mbart\" in model_checkpoint:\n","    tokenizer.src_lang = \"en-XX\"\n","    tokenizer.tgt_lang = \"ro-RO\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5hBlsrHIrJL","outputId":"acdaa98a-a8cd-4a20-89b8-cc26437bbe90"},"outputs":[{"data":{"text/plain":["{'input_ids': [4708, 2, 69, 28, 9, 8662, 84, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(\"Hello, this is a sentence!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bYY_qACIu0qT","outputId":"c7d9a4e1-782a-4daf-9a60-036c8e818077"},"outputs":[{"data":{"text/plain":["{'input_ids': [[4708, 2, 69, 28, 9, 8662, 84, 0], [188, 28, 823, 8662, 3, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer([\"Hello, this is a sentence!\", \"This is another sentence.\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHzLFwe3u0qU","outputId":"85c25912-3da2-48ec-d127-cfac9d1cd7b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [[14232, 244, 2, 69, 160, 6, 9, 10513, 1101, 84, 0], [13486, 6, 160, 6, 3778, 4853, 10513, 1101, 3, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"]}],"source":["with tokenizer.as_target_tokenizer():\n","    print(tokenizer([\"Hello, this is a sentence!\", \"This is another sentence.\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdHdUeVlu0qU"},"outputs":[],"source":["if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n","    prefix = \"translate English to Romanian: \"\n","else:\n","    prefix = \"\""]},{"cell_type":"markdown","metadata":{"id":"Gi3oKsj8u0qU"},"source":["We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vc0BSBLIIrJQ"},"outputs":[],"source":["max_input_length = 128\n","max_target_length = 128\n","source_lang = \"en\"\n","target_lang = \"ro\"\n","\n","\n","def preprocess_function(examples):\n","    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n","    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","\n","    # Setup the tokenizer for targets\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"markdown","metadata":{"id":"0lm8ozrJIrJR"},"source":["This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-b70jh26IrJS","outputId":"acd3a42d-985b-44ee-9daa-af5d944ce1d9"},"outputs":[{"data":{"text/plain":["{'input_ids': [[37284, 8, 949, 37, 358, 31483, 0], [32818, 8, 31483, 8, 2541, 7910, 37, 358, 31483, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[1163, 8008, 7037, 26971, 37, 9, 56, 16836, 9026, 226, 15, 33834, 0], [67, 16852, 791, 9026, 896, 15, 33834, 111, 10795, 9351, 26549, 11114, 37, 9, 56, 16836, 9026, 226, 15, 33834, 0]]}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["preprocess_function(raw_datasets[\"train\"][:2])"]},{"cell_type":"markdown","metadata":{"id":"zS-6iXTkIrJT"},"source":["To apply this function on all the pairs of sentences in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDtsaJeVIrJT","outputId":"aa4734bf-4ef5-4437-9948-2c16363da719"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /home/matt/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/28ebdf8cf22106c2f1e58b2083d4b103608acd7bfdb6b14313ccd9e5bc8c313a/cache-f1b4cc7f6a817a09.arrow\n","Loading cached processed dataset at /home/matt/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/28ebdf8cf22106c2f1e58b2083d4b103608acd7bfdb6b14313ccd9e5bc8c313a/cache-2dcbdf92c911af2a.arrow\n","Loading cached processed dataset at /home/matt/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/28ebdf8cf22106c2f1e58b2083d4b103608acd7bfdb6b14313ccd9e5bc8c313a/cache-34490b3ad1e70b86.arrow\n"]}],"source":["tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TlqNaB8jIrJW","outputId":"84916cf3-6e6c-47f3-d081-032ec30a4132"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-07-25 17:49:51.571462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-25 17:49:51.577820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-25 17:49:51.578841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-25 17:49:51.580434: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-07-25 17:49:51.583246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-25 17:49:51.583929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-25 17:49:51.584582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-25 17:49:51.938374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-25 17:49:51.939080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-25 17:49:51.939739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-25 17:49:51.940364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21659 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:21:00.0, compute capability: 8.6\n","2022-07-25 17:49:53.116600: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n","All model checkpoint layers were used when initializing TFMarianMTModel.\n","\n","All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-ROMANCE.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"]}],"source":["from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n","\n","model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bliy8zgjIrJY"},"outputs":[],"source":["batch_size = 16\n","learning_rate = 2e-5\n","weight_decay = 0.01\n","num_train_epochs = 1\n","\n","model_name = model_checkpoint.split(\"/\")[-1]\n","push_to_hub_model_id = f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ebNrB5rwu0qX"},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\")\n","\n","generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZtATNfNJu0qX"},"outputs":[],"source":["train_dataset = model.prepare_tf_dataset(\n","    tokenized_datasets[\"train\"],\n","    batch_size=batch_size,\n","    shuffle=True,\n","    collate_fn=data_collator,\n",")\n","\n","validation_dataset = model.prepare_tf_dataset(\n","    tokenized_datasets[\"validation\"],\n","    batch_size=batch_size,\n","    shuffle=False,\n","    collate_fn=data_collator,\n",")\n","\n","generation_dataset = model.prepare_tf_dataset(\n","    tokenized_datasets[\"validation\"],\n","    batch_size=8,\n","    shuffle=False,\n","    collate_fn=generation_data_collator,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0zwlp81u0qX","outputId":"b20669b2-20c8-463d-ce92-4b96b374fe1d"},"outputs":[{"name":"stderr","output_type":"stream","text":["No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"]}],"source":["from transformers import AdamWeightDecay\n","import tensorflow as tf\n","\n","optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n","model.compile(optimizer=optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IvCRKTI8u0qY"},"outputs":[],"source":["from transformers.keras_callbacks import KerasMetricCallback\n","import numpy as np\n","\n","\n","def metric_fn(eval_predictions):\n","    preds, labels = eval_predictions\n","    prediction_lens = [\n","        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n","    ]\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","\n","    # We use -100 to mask labels - replace it with the tokenizer pad token when decoding\n","    # so that no output is emitted for these\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Some simple post-processing\n","    decoded_preds = [pred.strip() for pred in decoded_preds]\n","    decoded_labels = [[label.strip()] for label in decoded_labels]\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n","    result = {\"bleu\": result[\"score\"]}\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    return result\n","\n","\n","metric_callback = KerasMetricCallback(\n","    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True,\n","    generate_kwargs={\"max_length\": 128}\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"ybZ9Rh99u0qY","outputId":"32016e2c-af11-4df4-84ce-3ae51daa145c"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/matt/PycharmProjects/notebooks/examples/translation_model_save is already a clone of https://huggingface.co/Rocketknight1/opus-mt-en-ROMANCE-finetuned-en-to-ro. Make sure you pull the latest changes with `repo.git_pull()`.\n"]},{"name":"stdout","output_type":"stream","text":["    6/38145 [..............................] - ETA: 56:25 - loss: 5.2187WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0788s vs `on_train_batch_end` time: 0.1046s). Check your callbacks.\n","38145/38145 [==============================] - ETA: 0s - loss: 0.7140"]},{"name":"stderr","output_type":"stream","text":["2022-07-25 18:43:16.811498: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x5633dc97b3a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2022-07-25 18:43:16.811529: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n","2022-07-25 18:43:16.943241: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","2022-07-25 18:43:17.816234: I tensorflow/compiler/xla/service/dynamic_dimension_inference.cc:965] Reshaping a dynamic dimension into a scalar, which has undefined behavior when input size is 0. The offending instruction is: %reshape.41 = s32[] reshape(s32[<=1]{0} %set-dimension-size.3), metadata={op_type=\"Equal\" op_name=\"cond/while/map/while/map/while/cond/cond/Equal\" source_file=\"/home/matt/PycharmProjects/transformers/src/transformers/generation_tf_logits_process.py\" source_line=351}\n","2022-07-25 18:43:25.655895: I tensorflow/compiler/jit/xla_compilation_cache.cc:478] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","2022-07-25 18:45:51.416864: I tensorflow/compiler/xla/service/dynamic_dimension_inference.cc:965] Reshaping a dynamic dimension into a scalar, which has undefined behavior when input size is 0. The offending instruction is: %reshape.41 = s32[] reshape(s32[<=1]{0} %set-dimension-size.3), metadata={op_type=\"Equal\" op_name=\"cond/while/map/while/map/while/cond/cond/Equal\" source_file=\"/home/matt/PycharmProjects/transformers/src/transformers/generation_tf_logits_process.py\" source_line=351}\n","Several commits (2) will be pushed upstream.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r","38145/38145 [==============================] - 3382s 88ms/step - loss: 0.7140 - val_loss: 1.2757 - bleu: 26.7914 - gen_len: 41.4932\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f4af02c52d0>"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["from transformers.keras_callbacks import PushToHubCallback\n","from tensorflow.keras.callbacks import TensorBoard\n","\n","tensorboard_callback = TensorBoard(log_dir=\"./translation_model_save/logs\")\n","\n","push_to_hub_callback = PushToHubCallback(\n","    output_dir=\"./translation_model_save\",\n","    tokenizer=tokenizer,\n","    hub_model_id=push_to_hub_model_id,\n",")\n","\n","callbacks = [metric_callback, tensorboard_callback, push_to_hub_callback]\n","\n","model.fit(\n","    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["2ad4b75e5c8f442d8f1492d46d934506","a32e7310f642425991fa40022620c199","d852fec625c7450fb8d284ca6518f413","f45af3274b5c4e50a68ee2c77f6af94a","bb06b7aa33f54896950a8d1eb1245c21","26017a377a3d4a04add74085af38ebf4","5ab4277b7cc24e9d8ebbdc0280c6bda8"]},"id":"YWF9Z839u0qZ","outputId":"75bb9ac2-47c8-42d1-f5a0-5632dda774bb"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ad4b75e5c8f442d8f1492d46d934506","version_major":2,"version_minor":0},"text/plain":["Downloading tokenizer_config.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a32e7310f642425991fa40022620c199","version_major":2,"version_minor":0},"text/plain":["Downloading source.spm:   0%|          | 0.00/761k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d852fec625c7450fb8d284ca6518f413","version_major":2,"version_minor":0},"text/plain":["Downloading target.spm:   0%|          | 0.00/780k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f45af3274b5c4e50a68ee2c77f6af94a","version_major":2,"version_minor":0},"text/plain":["Downloading vocab.json:   0%|          | 0.00/1.51M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb06b7aa33f54896950a8d1eb1245c21","version_major":2,"version_minor":0},"text/plain":["Downloading special_tokens_map.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26017a377a3d4a04add74085af38ebf4","version_major":2,"version_minor":0},"text/plain":["Downloading config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ab4277b7cc24e9d8ebbdc0280c6bda8","version_major":2,"version_minor":0},"text/plain":["Downloading tf_model.h5:   0%|          | 0.00/298M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2022-07-26 17:56:38.238360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-26 17:56:38.275342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-26 17:56:38.276357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-26 17:56:38.278209: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-07-26 17:56:38.309314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-26 17:56:38.310572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-26 17:56:38.311790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-26 17:56:39.033228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-26 17:56:39.033908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-26 17:56:39.034535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-26 17:56:39.035152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21719 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:21:00.0, compute capability: 8.6\n","2022-07-26 17:56:40.631257: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n","All model checkpoint layers were used when initializing TFMarianMTModel.\n","\n","All the layers of TFMarianMTModel were initialized from the model checkpoint at Rocketknight1/opus-mt-en-ROMANCE-finetuned-en-to-ro.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"]}],"source":["from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n","\n","# You can of course substitute your own username and model here if you've trained and uploaded it!\n","model_name = 'Rocketknight1/opus-mt-en-ROMANCE-finetuned-en-to-ro'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8p07Dxgsu0qZ","outputId":"1db4a730-f1d9-4ca4-e30a-3cc9ab8433e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(\n","[[65000   642  1204     5 12648    35 26792   415 36773  5031 11008   208\n","      2  1019   203  2836   600   229 15032  3796 13286   226     3     0\n","  65000 65000 65000 65000 65000 65000 65000 65000 65000 65000 65000 65000\n","  65000 65000 65000 65000 65000 65000 65000 65000 65000 65000 65000 65000\n","  65000 65000 65000 65000 65000 65000 65000 65000 65000 65000 65000 65000\n","  65000 65000 65000 65000 65000 65000 65000 65000 65000 65000 65000 65000\n","  65000 65000 65000 65000 65000 65000 65000 65000 65000 65000 65000 65000\n","  65000 65000 65000 65000 65000 65000 65000 65000 65000 65000 65000 65000\n","  65000 65000 65000 65000 65000 65000 65000 65000 65000 65000 65000 65000\n","  65000 65000 65000 65000 65000 65000 65000 65000 65000 65000 65000 65000\n","  65000 65000 65000 65000 65000 65000 65000 65000]], shape=(1, 128), dtype=int32)\n"]}],"source":["input_text  = \"I'm not actually a very competent Romanian speaker, but let's try our best.\"\n","if 't5' in model_name:\n","    input_text = \"translate English to Romanian: \" + input_text\n","tokenized = tokenizer([input_text], return_tensors='np')\n","out = model.generate(**tokenized, max_length=128)\n","print(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4IcWXYd1u0qZ","outputId":"e7f0db5a-0340-474c-be52-845696361c8e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Nu sunt de fapt un vorbitor român foarte competent, dar haideţi să facem tot posibilul.\n"]}],"source":["with tokenizer.as_target_tokenizer():\n","    print(tokenizer.decode(out[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mV5QCJKHu0qb","outputId":"10e4167e-2327-4b39-c6e9-5700ce2cdcd5"},"outputs":[{"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFMarianMTModel.\n","\n","All the layers of TFMarianMTModel were initialized from the model checkpoint at Rocketknight1/opus-mt-en-ROMANCE-finetuned-en-to-ro.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"]}],"source":["from transformers import pipeline\n","\n","translator = pipeline('text2text-generation', model_name, framework=\"tf\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EpaV3vtu0qb","outputId":"b524e932-5dac-4a92-a4f3-28b29f0138b5"},"outputs":[{"data":{"text/plain":["[{'generated_text': 'Nu sunt de fapt un vorbitor român foarte competent, dar haideţi să facem tot posibilul.'}]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["translator(input_text, max_length=128)"]},{"cell_type":"markdown","metadata":{"id":"yetrejk8u0qc"},"source":["Easy!"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}